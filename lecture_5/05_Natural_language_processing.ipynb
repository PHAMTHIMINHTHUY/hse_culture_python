{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 05. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Сегодня мы с вами все же добрались до самого интересного -- автоматической обработки естественного языка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Привет, друзья! Сегодня в галлерее 11.12 в USERTAG открывается выставка нашего друга художника USERTAG. \n",
    "Вокруг нашей песни “1999” Слава и режиссер Сергей Канчер создали большой проект, посвященный Первой и Второй Чеченским войнам. \n",
    "Это и тотальная инсталляция, и фотографии, и артефакты, и интервью с ветеранами обеих войн и, конечно, видео на песню “1999”, которое вы впервые сможете увидеть на выставке. \n",
    "Это попытка осмыслить жуткий опыт войны. \n",
    "Это попытка вглядеться в то, на что нельзя закрывать глаза. \n",
    "Потому что, даже если отвернуться, то, что произошло, никуда не денется.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ',', 'друзья', '!', 'Сегодня', 'в', 'галлерее', '11.12', 'в', 'USERTAG', 'открывается', 'выставка', 'нашего', 'друга', 'художника', 'USERTAG', '.', 'Вокруг', 'нашей', 'песни', '“', '1999', '”', 'Слава', 'и', 'режиссер', 'Сергей', 'Канчер', 'создали', 'большой', 'проект', ',', 'посвященный', 'Первой', 'и', 'Второй', 'Чеченским', 'войнам', '.', 'Это', 'и', 'тотальная', 'инсталляция', ',', 'и', 'фотографии', ',', 'и', 'артефакты', ',', 'и', 'интервью', 'с', 'ветеранами', 'обеих', 'войн', 'и', ',', 'конечно', ',', 'видео', 'на', 'песню', '“', '1999', '”', ',', 'которое', 'вы', 'впервые', 'сможете', 'увидеть', 'на', 'выставке', '.', 'Это', 'попытка', 'осмыслить', 'жуткий', 'опыт', 'войны', '.', 'Это', 'попытка', 'вглядеться', 'в', 'то', ',', 'на', 'что', 'нельзя', 'закрывать', 'глаза', '.', 'Потому', 'что', ',', 'даже', 'если', 'отвернуться', ',', 'то', ',', 'что', 'произошло', ',', 'никуда', 'не', 'денется', '.']\n"
     ]
    }
   ],
   "source": [
    "words_nltk = word_tokenize(text)\n",
    "print(words_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сравним с токенизатором, написанным вручную (с помощью регулярок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ',', 'друзья', '!', 'Сегодня', 'в', 'галлерее', '11', '.', '12', 'в', 'USERTAG', 'открывается', 'выставка', 'нашего', 'друга', 'художника', 'USERTAG', '.', 'Вокруг', 'нашей', 'песни', '“1999”', 'Слава', 'и', 'режиссер', 'Сергей', 'Канчер', 'создали', 'большой', 'проект', ',', 'посвященный', 'Первой', 'и', 'Второй', 'Чеченским', 'войнам', '.', 'Это', 'и', 'тотальная', 'инсталляция', ',', 'и', 'фотографии', ',', 'и', 'артефакты', ',', 'и', 'интервью', 'с', 'ветеранами', 'обеих', 'войн', 'и', ',', 'конечно', ',', 'видео', 'на', 'песню', '“1999”', ',', 'которое', 'вы', 'впервые', 'сможете', 'увидеть', 'на', 'выставке', '.', 'Это', 'попытка', 'осмыслить', 'жуткий', 'опыт', 'войны', '.', 'Это', 'попытка', 'вглядеться', 'в', 'то', ',', 'на', 'что', 'нельзя', 'закрывать', 'глаза', '.', 'Потому', 'что', ',', 'даже', 'если', 'отвернуться', ',', 'то', ',', 'что', 'произошло', ',', 'никуда', 'не', 'денется', '.']\n"
     ]
    }
   ],
   "source": [
    "words_re = [i.strip() for i in re.split(r\"([ \\.\\?!,]{1,})\", text) if i.strip()]\n",
    "print(words_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_nltk == words_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как проверить, в чем отличие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Деление на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nltk = sent_tokenize(text)\n",
    "print(sentences_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_re = [i.strip() for i in re.split(r\"(.+?[\\.!\\?]{1,}\\s)\", text) if i.strip()]\n",
    "print(sentences_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nltk == sentences_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если взять наш старый текст, посложнее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"13 сентября будем ходить ходуном. Прямо во дворе Пауэрхауса. Всё, как обычно, только соскучившись. Новые песни, старые песни. Прыжки и кувырки. Радость и смех.\n",
    "Такое надо в корне пресекать!\n",
    "Билеты: https://sbp4band.ticketscloud.org\n",
    "\n",
    "Пожалуйста, планируйте приобретение билетов заранее. Высока вероятность, что продажа на входе осуществляться не будет\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nltk = sent_tokenize(text)\n",
    "print(sentences_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_re = [i.strip() for i in re.split(r\"(.+?[\\.!\\?]{1,}\\s)\", text) if i.strip()]\n",
    "print(sentences_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nltk == sentences_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NB! иногда ваш код может быть лучше чего-то готового_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    m = Mystem()\n",
    "    lemmas = m.lemmatize(text)\n",
    "    lemmatized_text = ''.join(lemmas)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# НАТАААШАААА\n",
    "\n",
    "https://github.com/natasha/natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![natasha](https://www.meme-arsenal.com/memes/f3c9f08ab239e223f48647b2826a1538.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (Segmenter, MorphVocab, NewsEmbedding, \n",
    "NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, PER, NamesExtractor, Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(text)\n",
    "doc.segment(segmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.text for i in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i.text for i in doc.tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_morph(morph_tagger)\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i.lemma for i in doc.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Извлечение именованных сущностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нуууу.... может Наташа хотя бы имена выделять умеет? или города/места?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PER = person\n",
    "\n",
    "LOC = location\n",
    "\n",
    "ORG = organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_ner(ner_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for span in doc.spans:\n",
    "    print(text[span.start:span.stop], span.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "давайте не будем отчаиваться и попробуем найти все места / локации во всех текстах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = set()\n",
    "persons = set()\n",
    "organizations = set()\n",
    "\n",
    "with open(\"texts.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        text = line.strip()\n",
    "        \n",
    "        doc = Doc(text)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.tag_ner(ner_tagger)\n",
    "        \n",
    "        for span in doc.spans:\n",
    "            span.normalize(morph_vocab)\n",
    "        \n",
    "            if span.type == 'LOC':\n",
    "                locations.add(span.normal)\n",
    "            \n",
    "            elif span.type == 'PER':\n",
    "                persons.add(span.normal)\n",
    "            \n",
    "            elif span.type == 'ORG':\n",
    "                organizations.add(span.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(organizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, мы видим, что результаты можно и нужно подчистить: удалить кавычки, смайлики, лишние знаки..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment analysis\n",
    "\n",
    "![](https://camo.githubusercontent.com/8aa4db250051d2819683668ad27cf90862c644e2/68747470733a2f2f692e696d6775722e636f6d2f754c4d5750754c2e706e67)\n",
    "\n",
    "https://github.com/bureaucratic-labs/dostoevsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека классифицирует текст на 5 категорий:\n",
    "\n",
    "* Негативное настроение; \n",
    "* Позитивное настроение;\n",
    "* Нейтральное поведение;\n",
    "* Речевой акт (формальные поздравления, благодарственные и поздравительные посты);\n",
    "* Класс «пропустить» для неясных случаев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dostoevsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m dostoevsky download fasttext-social-network-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastTextSocialNetworkModel(tokenizer=RegexTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    'привет',\n",
    "    'я люблю тебя!!',\n",
    "    'малолетние дебилы'\n",
    "]\n",
    "\n",
    "results = model.predict(messages)\n",
    "\n",
    "for message, sentiment in zip(messages, results):\n",
    "    print(message, '->', sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "оооокей, а давайте посмотрим на наши тексты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"13 сентября будем ходить ходуном. Прямо во дворе Пауэрхауса. Всё, как обычно, только соскучившись. Новые песни, старые песни. Прыжки и кувырки. Радость и смех.\n",
    "Такое надо в корне пресекать!\n",
    "Билеты: https://sbp4band.ticketscloud.org\n",
    "\n",
    "Пожалуйста, планируйте приобретение билетов заранее. Высока вероятность, что продажа на входе осуществляться не будет\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"Привет, друзья! Сегодня в галлерее 11.12 в USERTAG открывается выставка нашего друга художника USERTAG. \n",
    "Вокруг нашей песни “1999” Слава и режиссер Сергей Канчер создали большой проект, посвященный Первой и Второй Чеченским войнам. \n",
    "Это и тотальная инсталляция, и фотографии, и артефакты, и интервью с ветеранами обеих войн и, конечно, видео на песню “1999”, которое вы впервые сможете увидеть на выставке. \n",
    "Это попытка осмыслить жуткий опыт войны. \n",
    "Это попытка вглядеться в то, на что нельзя закрывать глаза. \n",
    "Потому что, даже если отвернуться, то, что произошло, никуда не денется.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict([text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
